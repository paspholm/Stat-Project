```{r hw3_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)

library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rvest)
tidymodels_prefer()
library(mosaic)
library(glmnet)
```

# Homework 3 {-}

<center>
**Due Wednesday, October 20th at 11:59pm on [Moodle](https://moodle.macalester.edu/mod/assign/view.php?id=27981)**
</center>

**Deliverables:** Please use [this template](template_rmds/hw3.Rmd) to knit an HTML document. Convert this HTML document to a PDF by opening the HTML document in your web browser. *Print* the document (Ctrl/Cmd-P) and change the destination to "Save as PDF". Submit this one PDF to Moodle.

Alternatively, you may knit your Rmd directly to PDF if you have LaTeX installed.



<br><br><br>



## Project Work {-}

(Note: This includes HW2 investigations plus a few tasks for dealing with non-linearity.)

**Goal:** Begin an analysis of your dataset to answer your **regression** research question.

<br>

**Collaboration:** Form a team (2-3 members) for the project and this part can be done as a team. Only one team member should submit a Project Work section. Make sure you include the full names of all of the members in your write up. 

<br>

**Data cleaning:** If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the [R Resources page](r-resources.html) to see if your questions are answered there. If not, post on the #rcode-questions channel in our Slack workspace to ask for help. *Please ask for help early and regularly* to avoid stressful workloads.

```{r}
pitching = read.csv("stats.csv")


pitching_clean = pitching %>%
  select(-c(118:139))
  
  
pitching_clean[is.na(pitching_clean)]<- 0
```

```{r}
set.seed(123)
pitching_cv_11 <- vfold_cv(pitching_clean, v = 11)
```

```{r}
#ols 
lm_spec <- 
    linear_reg() %>% # this is the type of model we are fitting
    set_engine(engine = 'lm') %>% # you'll learn other engines to fit the model
    set_mode('regression') 

#lasso model
lm_lasso_spec <- 
  linear_reg()%>%
  set_args(mixture = 1, penalty = tune())%>%
  set_engine(engine = 'glmnet')%>%
  set_mode('regression')
```

```{r}
# recipes & workflows OLS
season_rec <- recipe(p_k_percent ~ ., data = pitching_clean) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())#remove predictors with near zero variablility

season_model_wf <- workflow() %>%
  add_recipe(season_rec)%>%
  add_model(lm_spec)

#recipe and workflow for LASSO
season_rec_LASSO <-
  recipe(p_k_percent ~ ., data = pitching_clean) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())%>%#remove predictors with near zero variablility
  step_normalize(all_numeric_predictors())%>%#normalize all predictors 
  step_corr(all_numeric_predictors())


# Lasso Model Spec with tune
lm_lasso_spec_tune <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 
```


```{r}
# fit & tune models

# Workflow (Recipe + Model)
lasso_wf_tune <- workflow() %>% 
  add_recipe(season_rec_LASSO) %>%
  add_model(lm_lasso_spec_tune) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 3)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning hyperparameters
  lasso_wf_tune, # workflow
  resamples = pitching_cv_11, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

autoplot(tune_output)
+theme_classic()
```

```{r}
#  calculate/collect CV metrics
Season_CV_metrics <-fit_resamples(season_model_wf, 
    resamples = pitching_cv_11, 
    metrics = metric_set(rmse, rsq, mae)
    ) %>%
    collect_metrics(summarize = TRUE) #CV Metrics (averages over the 10 folds)

Season_CV_metrics
```




<br>


**Required Analyses:**

1. **Initial investigation: ignoring nonlinearity (for now)**
    a. Use ordinary least squares (OLS) by using the `lm` engine and LASSO (`glmnet` engine) to build  a series of initial regression models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don't want to consider as predictors.)
        - You'll need two model specifications, `lm_spec` and `lm_lasso_spec` (you'll need to tune this one).
    b. For each set of variables, you'll need a `recipe` with the `formula`, `data`, and pre-processing steps
        - You may want to have steps in your recipe that remove variables with near zero variance (`step_nzv()`), remove variables that are highly correlated with other variables (`step_corr()`), normalize all quantitative predictors (`step_normalize(all_numeric_predictors())`) and add indicator variables for any categorical variables (`step_dummy(all_nominal_predictors())`).
        - These models should not include any transformations to deal with nonlinearity. You'll explore this in the next investigation.
    c. Estimate the test performance of the models using CV. Report and interpret (with units) the CV metric estimates along with a measure of uncertainty in the estimate (`std_error` is readily available when you used `collect_metrics(summarize=TRUE)`).
        - Compare estimated test performance across the models. Which models(s) might you prefer?
    d. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.
    e. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising?
        - Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.
        
Note: after this process, you might have a set of models (one of which has predictors chosen using LASSO, one model with all the predictors of interest, and perhaps some models with subsets of predictors that were chosen based on the data context rather than an algorithmic process)
<br>

2. **Accounting for nonlinearity**
    - Update your models to use natural splines for some of the quantitative predictors to account for non-linearity (these are GAMs).
        - I recommend using OLS engine to fit these final models.
        - You'll need to update the recipe to include `step_ns()` for each quantitative predictor that you want to allow to be non-linear.
        - To determine number of knots (`deg_free`), I recommend fitting a smoothing spline and use `edf` to inform your choice.

    - Compare insights from variable importance analyses here and the corresponding results from the Investigation 1. Now after having accounted for nonlinearity, have the most relevant predictors changed?
        - Do you gain any insights from the GAM output plots (easily obtained from fitting smoothing splines) for each predictor?
        
    - Compare model performance between your GAM models that the models that assuming linearity.
        - How does test performance of the GAMs compare to other models you explored?

    - Don't worry about KNN for now.

<br>

3. **Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?

<br>

4. **Societal impact**
    - Are there any harms that may come from your analyses and/or how the data were collected?
    - What cautions do you want to keep in mind when communicating your work?



<br><br><br>
