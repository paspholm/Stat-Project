---
title: "hw4"
output: html_document
---

```{r}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)

library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rvest)
tidymodels_prefer()
library(mosaic)
library(glmnet)
```


# Reseach Question

Using pitch metrics to predict strikeout percentage

```{r, eval=FALSE}
pitching = read.csv("stats.csv")


pitching_clean = pitching %>%
  select(c(1,2,7,34,35,36,118:138)) %>%
  drop_na() %>%
  select(-c(1,2)) %>%
  select(!contains("formatted")) %>%
  mutate(over25kperc = factor(ifelse(p_k_percent > 25, 1, 0))) %>%
  select(-c(1))

# save(pitching_clean, file = "pitching_clean.rdata")

```

```{r}
load("pitching_clean.rdata")
```


# Logistic Regression

```{r}
set.seed(69)

pitching_cv11 = vfold_cv(pitching_clean, v = 11)

# model specification
logistic_lasso_spec_tune = logistic_reg() %>%
  set_engine("glmnet") %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_mode('classification')

# recipe
logistic_rec = recipe(over25kperc ~ ., data = pitching_clean) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

# workflow
log_lasso_wf = workflow() %>%
  add_recipe(logistic_rec) %>%
  add_model(logistic_lasso_spec_tune)

# Tune Model
penalty_grid = grid_regular(
  penalty(range = c(-5, 0)), 
  levels = 30
)

tune_output = tune_grid(
  log_lasso_wf,
  resamples = pitching_cv11,
  metrics = metric_set(roc_auc),
  control = control_resamples(save_pred = TRUE, event_level = "second"),
  grid = penalty_grid
)

autoplot(tune_output) + theme_classic()

best_se_penalty = select_by_one_std_err(tune_output, metric = "roc_auc", desc(penalty))

best_se_penalty


```

```{r}
#final fit
final_fit_se = finalize_workflow(log_lasso_wf, best_se_penalty)%>%
  fit(data = pitching_clean)

tidy(final_fit_se)
```

```{r}
#variable importance 

glmnet_output <- final_fit_se %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    this_coeff_path <- bool_predictor_exclude[row,]
    if(mosaic::sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{
    return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)}
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```



```{r}
tune_output%>% 
  collect_metrics()%>%
  filter(penalty == best_se_penalty %> %pull(penalty))

pitching_clean%>%
  count(over25kperc)

logistic_output <-  pitching_clean %>%
  bind_cols(predict(final_fit_se, new_data = pitching_clean, type = 'prob')) 

logistic_output%>%
  ggplot(aes(y = .pred_1, x = over25kperc))+
  geom_boxplot()+theme_classic()


```



```{r}
#hard predicitons
logistic_output %>%
  ggplot(aes(x = over25kperc, y = .pred_0))+
  geom_boxplot()+
  geom_hline(yintercept = 0.69, col = "red")+
  theme_classic()

logistic_output<-logistic_output%>%
  mutate(.pred_class = make_two_class_pred(.pred_0, levels(over25kperc), threshold = .69))
  
head(logistic_output)

logistic_output%>%
  count(over25kperc, .pred_class)

logistic_output %>%
  conf_mat(truth = over25kperc, estimate = .pred_class)
```




# Decision Trees/ Random Forests
```{r}
set.seed(123)

#model specification
rf_spec <- rand_forest()%>%
  set_engine(engine = 'ranger') %>%
  set_args(mtry = NULL,
           trees = 500, #Number of trees
           min_n = 2, #minimum number of points in a leaf?
           probability = FALSE,
           importance = 'impurity') %>%
  set_mode('classification')

#recipe
data_rec <- recipe(over25kperc ~ ., data = pitching_clean)

#workflows
data_wf_mtry10 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 10))%>%
  add_recipe(data_rec)

```

```{r}
# fit model
set.seed(123)
data_fit_mtry10 <- fit(data_wf_mtry10, data = pitching_clean_rf)

rf_OOB_output <- function(fit_model, model_label, truth){
  tibble(
    .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), 
    class = truth,
    label = model_label
  
  )
}

#check the output
rf_OOB_output(data_fit_mtry10,data_fit_mtry10$over25kperc, pitching_clean_rf %>% pull(over25kperc))

# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry10,data_fit_mtry10$over25kperc, pitching_clean_rf %>% pull(over25kperc)))

data_rf_OOB_output %>% 
  accuracy(truth = class, estimate = .pred_class) 
```
```{r}
data_fit_mtry10
rf_OOB_output(data_fit_mtry10, data_fit_mtry10$over25kperc, pitching_clean_rf %>% pull(over25kperc)) %>%
  conf_mat(truth = class, estimate = .pred_class)

sensitivity = 77/(77+48)
specificity = 266/(266+16)
sensitivity
specificity
```
Our sensitivity is .616 and our specificity is .943. Therefore, our model is better at predicting when pitchers are below a 25% strikeout rate than above a 25% strikeout rate - it is better at predicting true negatives than true positives. We think this might be happening because pitchers that have a strikeout rate above 25% are very hard to come by - they are a somewhat elite group, even though the statistic is only slightly above the mlb average. Additionally, our data does not include relief pitchers, who tend to rely more on striking batters out than starting pitchers, who might rely on generating ground balls and soft contact.

```{r}
# evaluate predictor performance based on permutation
model_output <- data_wf_mtry10 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = pitching_clean_rf) %>% 
    extract_fit_engine() 

model_output %>% 
    vip(num_features = 30) + theme_classic()


model_output %>% vip::vi() %>% head()
model_output %>% vip::vi() %>% tail()
```
The most important variables in our model are the average speed and average spin rate of a pitcher's four-seam fastball. This makes sense because this is a pitch that most, if not all, pitchers use.












