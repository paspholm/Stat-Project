---
title: "Final Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rvest)
tidymodels_prefer()
library(mosaic)
library(glmnet)
library(probably)
library(vip)
conflicted::conflict_prefer("vi", "vip")

load("pitching_clean.rdata")
```

## LASSO OLS Model 


```{r}
set.seed(123)
pitching_cv_11 <- vfold_cv(pitching_clean, v = 11)
```

```{r}
#ols  
lm_spec <- 
    linear_reg() %>% # this is the type of model we are fitting
    set_engine(engine = 'lm') %>% # you'll learn other engines to fit the model
    set_mode('regression') 

#lasso model
lm_lasso_spec <- 
  linear_reg()%>%
  set_args(mixture = 1, penalty = tune())%>%
  set_engine(engine = 'glmnet')%>%
  set_mode('regression')
```

```{r}
# recipes & workflows OLS
season_rec <- recipe(p_k_percent ~ ., data = pitching_clean) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())#remove predictors with near zero variablility

season_model_wf <- workflow() %>%
  add_recipe(season_rec)%>%
  add_model(lm_spec)

#recipe and workflow for LASSO
season_rec_LASSO <-
  recipe(p_k_percent ~ ., data = pitching_clean) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())%>%#remove predictors with near zero variablility
  step_normalize(all_numeric_predictors())%>%#normalize all predictors 
  step_corr(all_numeric_predictors())


# Lasso Model Spec with tune
lm_lasso_spec_tune <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 
```


```{r}
# fit & tune models

# Workflow (Recipe + Model)
lasso_wf_tune <- workflow() %>% 
  add_recipe(season_rec_LASSO) %>%
  add_model(lm_lasso_spec_tune) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 3)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning hyperparameters
  lasso_wf_tune, # workflow
  resamples = pitching_cv_11, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

autoplot(tune_output) ### this isn't working 
+theme_classic()
```

```{r}
#  calculate/collect CV metrics
Season_CV_metrics <-fit_resamples(season_model_wf, 
    resamples = pitching_cv_11, 
    metrics = metric_set(rmse, rsq, mae)
    ) %>%
    collect_metrics(summarize = TRUE) #CV Metrics (averages over the 10 folds)

Season_CV_metrics
```

## GAMS and Splines NEED TO DO 
## Logistic Regression: Question can we predict strikeout percentage with pitching metrics?  


```{r}
set.seed(69)

pitching_cv11 = vfold_cv(pitching_clean, v = 11)

# model specification
logistic_lasso_spec_tune = logistic_reg() %>%
  set_engine("glmnet") %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_mode('classification')

# recipe
logistic_rec = recipe(over25kperc ~ ., data = pitching_clean[-c(1,2)]) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

# workflow
log_lasso_wf = workflow() %>%
  add_recipe(logistic_rec) %>%
  add_model(logistic_lasso_spec_tune)

# Tune Model
penalty_grid = grid_regular(
  penalty(range = c(-5, -2)), 
  levels = 30
)

tune_output = tune_grid(
  log_lasso_wf,
  resamples = pitching_cv11,
  metrics = metric_set(roc_auc),
  control = control_resamples(save_pred = TRUE, event_level = "second"),
  grid = penalty_grid
)

autoplot(tune_output) + theme_classic()

best_se_penalty = select_by_one_std_err(tune_output, metric = "roc_auc", desc(penalty))

best_se_penalty
```

```{r}
#final fit
final_fit_se = finalize_workflow(log_lasso_wf, best_se_penalty)%>%
  fit(data = pitching_clean)

tidy(final_fit_se)
```

```{r}
#variable importance 

glmnet_output <- final_fit_se %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    this_coeff_path <- bool_predictor_exclude[row,]
    if(mosaic::sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{
    return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)}
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```

```{r}
tune_output%>% 
  collect_metrics()%>%
  filter(penalty == best_se_penalty %>%pull(penalty))

pitching_clean%>%
  count(over25kperc)

logistic_output <-  pitching_clean %>%
  bind_cols(predict(final_fit_se, new_data = pitching_clean, type = 'prob')) 

logistic_output%>%
  ggplot(aes(y = .pred_1, x = over25kperc))+
  geom_boxplot()+theme_classic()

```

```{r}
#hard predicitons
logistic_output %>%
  ggplot(aes(x = over25kperc, y = .pred_0))+
  geom_boxplot()+
  geom_hline(yintercept = 0.69, col = "red")+
  theme_classic()

logistic_output<-logistic_output%>%
  mutate(.pred_class = make_two_class_pred(.pred_0, levels(over25kperc), threshold = .69))
  
head(logistic_output)

logistic_output%>%
  count(over25kperc, .pred_class)

logistic_output %>%
  conf_mat(truth = over25kperc, estimate = .pred_class)
```


##Descision Trees/Random Forests 
```{r}
# Decision Trees/ Random Forests
set.seed(123)

#model specification
rf_spec <- rand_forest()%>%
  set_engine(engine = 'ranger') %>%
  set_args(mtry = NULL,
           trees = 500, #Number of trees
           min_n = 2, #minimum number of points in a leaf?
           probability = FALSE,
           importance = 'impurity') %>%
  set_mode('classification')

#recipe
data_rec <- recipe(over25kperc ~ ., data = pitching_clean_rf)

#workflows
data_wf_mtry10 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 10))%>%
  add_recipe(data_rec)
```

```{r}
# fit model
set.seed(123)
data_fit_mtry10 <- fit(data_wf_mtry10, data = pitching_clean_rf)

rf_OOB_output <- function(fit_model, model_label, truth){
  tibble(
    .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), 
    class = truth,
    label = model_label
  
  )
}

#check the output
rf_OOB_output(data_fit_mtry10,data_fit_mtry10$over25kperc, pitching_clean_rf %>% pull(over25kperc))

# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry10,data_fit_mtry10$over25kperc, pitching_clean_rf %>% pull(over25kperc)))

data_rf_OOB_output %>% 
  accuracy(truth = class, estimate = .pred_class) 
```

## Clustering 


Our goal with clustering is to use fastball pitching data to better explore the pitch and see if there are different types of fastballs. 

Our first step is to create a new dataframe containing only fastball data. Once that is done, we cluster from k = 1 to k = 15 to decide which k value to use.

```{r}
pitching_clean_cluster = pitching_clean %>%
  select(fastball_avg_spin, fastball_avg_speed, fastball_avg_break_x, fastball_avg_break_z)

pitching_cluster <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(pitching_clean_cluster), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, pitching_cluster)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()
```

Using the elbow method, we choose k = 3 as our number of clusters. We choose this because the total within-cluster sum of squares drop off begins to level out after 3.

```{r}
set.seed(123)
kclust_k6 = kmeans(scale(pitching_clean_cluster), centers = 3)
```

```{r}
pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_x, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) +
  geom_point() +
  xlab("Fastball Average Horizontal Break (Inches)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") + 
  theme_classic()
```

We first plot the average spin of the fastball vs the average horizontal break while coloring by cluster. We can see two clear clusters in this plot, pitches with break greater than 0, and pitches with break less than 0. Within each group the spin varies. This shows us that spin does not have a large effect on horizontal break, and that most pitchers' fastball move vertically, with very little pitchers located around 0 horizontal break. This plot motivates a deeper dive into the data as we see that cluster 2 and 3 are on top of eachother. 

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_z, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) +
  geom_point() +
  xlab("Fastball Average Vertical Break (Inches)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") +
  theme_classic()
```

Our next plot shows average spin vs the average vertical break. We see a better grouping of the clusters. With group 3 having a larger horizontal break, group 1 being more in the middle, and group 2 having the lowest vertical break. This is interesting because we see a clear difference between groups 2 and 3 in this plot. Group 3 has more vertical break whereas group 2 has the least vertical break.

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_speed, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) + 
  geom_point() +
  xlab("Fastball Average Speed (MPH)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") +
  theme_classic()

```

Our next graphic plots average spin with speed to see if we can see a clear clustering based on spin and speed. We see a similar result as the previous graph. Group 3 throws the hardest fastballs, followed by group 1, and group 2 being the slowest. We again see another interesting distinction between group 3 and group 2. Group 3 has both a higher average speed and vertical break whereas group 2 has the slowest average fastball and smallest vertical break.

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_x, fastball_avg_break_z, color = as.factor(kclust_k6$cluster))) + 
  geom_point() + 
  xlab("Fastball Average Horizontal Break (Inches)") +
  ylab("Fastball Average Vertical Break (Inches)") +
  labs(color = "Cluster") +
  theme_classic()
```

Our final plot shows a distinct 3 groups. Group 3 is has both a high vertical break and low horizontal break, group 2 has both low vertical and horizontal break, and group 1 has a high horizontal break. 

We can see that there are in fact 3 distinct types of fastball contained in our data. This corresponds with the type of pitches pitchers throw. Group 3 is the traditional 4-seam fastball which is thrown harder with more vertical break. Group 2 is a 2-seam/sinker which is thrown with more horizontal break but less speed. And group 1 is the cutter, which can range in velocities but moves in teh opposite direction of the 4-seam or 2-seam. 
