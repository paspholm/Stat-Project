---
title: "Final Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rvest)
tidymodels_prefer()
library(mosaic)
library(glmnet)
library(probably)
library(vip)
conflicted::conflict_prefer("vi", "vip")

load("pitching_clean.rdata")
```

## Data Context

Baseball is a sport that has been taken over by analytics. In recent year, new technologies have been able to track and quantify much of the action taking place on the field. This has given statisticians unprecedented access to explore how the sport works and how individuals are performing. 

We will analyze physical pitching metrics taken from Baseball Savant, which is an MLB run database containing all data collected by Statcast. Statcast collects physical measurements, such as pitch speed and exit velocity for example, during each event of each MLB game during the span The data was collected during 2017 to 2021 seasons.

Our final dataframe used for all analysis contains a summary of the pitches thrown by an individual pitcher for each season. It contains the total count of each fastball, offspeed, and breaking ball pitches. Each pich type has metrics explaining the average spin, horizontal break, vertical break, overall break, velocity, and range of the velocity. 

## LASSO OLS Model 


```{r}
pitching1 = read.csv("stats.csv")


pitching_reg_data <- pitching1 %>%
  select(c(1,2,7,34,35,36,118:138)) %>%
  select(-c(1,2)) %>%
  select(!contains("formatted")) %>%
  drop_na()
```


```{r}
set.seed(123)
pitching_cv_10 <- vfold_cv(pitching_reg_data, v = 10)
```

## Research question for OLS and LASSO linear regression: we are trying to predict strikeout percentage for a pitcher based on only pitch movement, spin and velocity. 

```{r}
#ols  
lm_spec <- 
    linear_reg() %>% # this is the type of model we are fitting
    set_engine(engine = 'lm') %>% # you'll learn other engines to fit the model
    set_mode('regression') 

#lasso model
lm_lasso_spec <- 
  linear_reg()%>%
  set_args(mixture = 1, penalty = tune())%>%
  set_engine(engine = 'glmnet')%>%
  set_mode('regression')
```

```{r}
# recipes & workflows OLS
pitcher_rec <- recipe(p_k_percent ~ ., data = pitching_reg_data) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())#remove predictors with near zero variablility

pitcher_wf <- workflow() %>%
  add_recipe(pitcher_rec)%>%
  add_model(lm_spec)

#recipe and workflow for LASSO
pitcher_rec_LASSO <-
  recipe(p_k_percent ~ ., data = pitching_reg_data) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())%>%#remove predictors with near zero variablility
  step_normalize(all_numeric_predictors())%>%#normalize all predictors 
  step_corr(all_numeric_predictors())


# Lasso Model Spec with tune
pitcher_wf_LASSO <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 
```


```{r}
# fit & tune models

# Workflow (Recipe + Model)
lasso_wf_tune <- workflow() %>% 
  add_recipe(pitcher_rec_LASSO) %>%
  add_model(pitcher_wf_LASSO) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 3)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning hyperparameters
  lasso_wf_tune, # workflow
  resamples = pitching_cv_10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

autoplot(tune_output)+theme_classic()
```

MAE and RMSE plotted against the amount of regularization, otherwise known as the LASSO penalty term. These plots show that as the penalty term increases, thus decreasing the amount of predictors, the cross validated error in our model will increase. At a certain point, after a penalty of 1, the error flatlines, meaning all predictors have been eliminated at this point. Our penalty of choice is probably around 0.3. We can confirm this by extracting the best penalty(shown in the code below), but also this is logical because going too low on the penalty will create problems with overfitting and going too high will have high training error. 


```{r}
#  calculate/collect CV metrics
pitching_cv_metrics <-fit_resamples(pitcher_wf, 
    resamples = pitching_cv_10, 
    metrics = metric_set(rmse,rsq, mae)
    ) %>%
    collect_metrics(summarize = TRUE) #CV Metrics (averages over the 10 folds)

pitching_cv_metrics

#lasso metrics

best_se_penalty <- select_by_one_std_err(tune_output, metric = 'mae', desc(penalty))

final_wf<- finalize_workflow(lasso_wf_tune, best_se_penalty)

final_fit <- fit(final_wf, data = pitching_reg_data)

tidy(final_fit)
```
The OLS model performs somewhat poorly, explaining only 50% of the variance in the data and having high error metrics. While this is not an unusable model, it would be desirable to have better test metrics. Residual plots could lead us to choosing nonlinear models or perhaps using clustering techniques. 

The LASSO output shows the predictors that are included in the final lasso model. The coefficients listed are the result of the LASSO penalty algorithm, where many coefficients are set to 0. This means that the addition of that predictor in the model did not decrease the error enough to justify the inclusion and corresponing increase in the penalty term. 
```{r}
#evaluating with residuals
pitcher_lasso_output <- final_fit %>%
  predict(new_data = pitching_reg_data)%>%
  bind_cols(pitching_reg_data)%>%
  mutate(resids = p_k_percent - .pred)

pitcher_lasso_output%>%
  rsq(truth = p_k_percent, estimate = .pred)

pitching_cv_mod <- fit(pitcher_wf, data = pitching_reg_data)

pitcher_cv_output <- pitching_cv_mod %>%
  predict(new_data = pitching_reg_data)%>%
  bind_cols(pitching_reg_data)%>%
  mutate(resids = p_k_percent - .pred)


```

```{r}
#residual plots OLS
ggplot(data = pitcher_cv_output, aes(x = p_k_percent, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
  
pitcher_cv_output%>%
  ggplot(aes(x = fastball_avg_spin, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_cv_output%>%
  ggplot(aes(x = pitch_count_fastball, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_cv_output%>%
  ggplot(aes(x = fastball_avg_speed, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
```

Residual plots from the OLS cross validated model. These plots show generally good residual plots, but the fastball count versus residual plot shows some heteroscedasticity. It seems that there is generally more error at lower numbers of fastball thrown per pitcher. 

```{r}
#residual plots lasso

  ggplot(data = pitcher_lasso_output, aes(x = p_k_percent, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
  
pitcher_lasso_output%>%
  ggplot(aes(x = fastball_avg_spin, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_lasso_output%>%
  ggplot(aes(x = pitch_count_fastball, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_lasso_output%>%
  ggplot(aes(x = fastball_avg_speed, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
```
The residual plots for the LASSO regression show a similar trend, with a slightly tighter fit and higher R^2 than the normal ols model. 


From this point, we think that testing nonlinear or classification model to decrease error is the best course of action. While the linear regressino models are not useless, the error estimates are simply too high for them to be legitimate predictive models that can be used in the future. 



<<<<<<< HEAD
## GAMS and Splines NEED TO DO 
## Research question for Logistic Regression: can we predict whether a pitcher has over or under a certain treshold (25%) strikeout percentage with velocity, movement and spin data? 
=======
## GAMS and Splines 

## Logistic Regression: Question: can we predict strikeout percentage with pitching metrics?  

>>>>>>> 93a38495881362acc223c97a6558451dd9a273e7

```{r}
set.seed(69)

pitching_cv11 = vfold_cv(pitching_clean, v = 11)

# model specification
logistic_lasso_spec_tune = logistic_reg() %>%
  set_engine("glmnet") %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_mode('classification')

# recipe
logistic_rec = recipe(over25kperc ~ ., data = pitching_clean[-c(1,2)]) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

# workflow
log_lasso_wf = workflow() %>%
  add_recipe(logistic_rec) %>%
  add_model(logistic_lasso_spec_tune)

# Tune Model
penalty_grid = grid_regular(
  penalty(range = c(-5, 3)), 
  levels = 30
)

tune_output = tune_grid(
  log_lasso_wf,
  resamples = pitching_cv11,
  metrics = metric_set(roc_auc),
  control = control_resamples(save_pred = TRUE, event_level = "second"),
  grid = penalty_grid
)

autoplot(tune_output) + theme_classic()

best_se_penalty = select_by_one_std_err(tune_output, metric = "roc_auc", desc(penalty))

best_se_penalty
```

For the LASSO in logistic regression, we used the ROC AUC metric for variable selection. This shows that as the lasso penalty increases, the AUC decreases. This means that the inclusion of more predictors is good for the model. 

With the LASSO penalty being used, the mean ROC AUC for our models was 0.87. This is metric is very high and indicates that our model is very accurate.


```{r}
#final fit
final_fit_se = finalize_workflow(log_lasso_wf, best_se_penalty)%>%
  fit(data = pitching_clean)

tidy(final_fit_se)
```

The output for our final model with all coefficients and predictors listed with the corresponding cross validated penalty. This shows us what predictors are included in our model. It seems that generally, the predictors are similar to those in the LASSO regression model. However, the performance of logistic regression is much better than our linear regression model. This indicates that our data may perform better under a classification framework rather than regression. 

```{r}
#variable importance 

glmnet_output <- final_fit_se %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    this_coeff_path <- bool_predictor_exclude[row,]
    if(mosaic::sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{
    return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)}
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```
Our variable importance table. This lists, in order, the variables that are most to least influential on our final model. Fastball avg speed and spin are equally the most important variables. This indicates that fastballs that are thrown hard with high spin rates are the biggest predictor of if someone has over 25% strikeout rate. 

```{r}
tune_output%>% 
  collect_metrics()%>%
  filter(penalty == best_se_penalty %>%pull(penalty))

pitching_clean%>%
  count(over25kperc)

logistic_output <-  pitching_clean %>%
  bind_cols(predict(final_fit_se, new_data = pitching_clean, type = 'prob')) 

logistic_output%>%
  ggplot(aes(y = .pred_0, x = over25kperc))+
  geom_boxplot()+theme_classic()

```
We can see the boxplots of our predictions for pitchers over and under 25% strikeout rate. There are a few outliers in the no column, meaning that there is a small group that does not strikeout many batters despite having metrics on their pitches that indicate a high strikeout rate. This could be due many factors, such as poor location, playing in a hitter friendly environment or pitch selection and game calling. 

<<<<<<< HEAD
```{r}
#hard predicitons
logistic_output %>%
  ggplot(aes(x = over25kperc, y = .pred_0))+
  geom_boxplot()+
  geom_hline(yintercept = 0.69, col = "red")+
  theme_classic()


log_metrics <- metric_set(accuracy, sens, yardstick::spec)

logistic_output %>%
  mutate(.pred_class = make_two_class_pred(.pred_0, levels(over25kperc), threshold= 0.65))%>%
  log_metrics(truth = over25kperc, estimate = .pred_class, event_level = 'second')

logistic_output %>%
  roc_curve(over25kperc, .pred_0, event_level = 'second')%>%
  autoplot()

threshold_output <- logistic_output %>%
  threshold_perf(truth = over25kperc, estimate = .pred_0, threshold = seq(0,1, by=0.01))

threshold_output %>%
  filter(.metric == "j_index")%>%
  ggplot(aes(x = .threshold, y = .estimate))+
  geom_line()+
  theme_classic()

threshold_output%>%
  filter(.metric == 'j_index')%>%
  arrange(desc(.estimate))
```
For our final logistic output model making a hard prediction, we use the j_index metric to choose the best threshold level in order to optimize accuracy, specificity and sensitivity. These metrics are output here along with the ROC_AUC graph and the boxplot with our hard prediction line superimposed. Overall, this model performs fairly well but could still be better. To some degree, we believe we are lacking data in certain areas that explain variance in strikout rate. Release height and angle could be important and deception is nearly impossible to account for quantitatively, but is generally accepted as an attribute that certain pitcher have, increasing their ability to strike hitters out. 


##Descision Trees/Random Forests 
=======
##Descision Trees/Random Forests
### Question: Predciting Strikeout precentage over 25% with random forests and decsions trees. 
=======
##Decision Trees/Random Forests 
>>>>>>> cc214bdae489cf23ea7eb757e47a915a44c14e0c
>>>>>>> 93a38495881362acc223c97a6558451dd9a273e7

```{r}
set.seed(123)

#model specification
rf_spec <- rand_forest()%>%
  set_engine(engine = 'ranger') %>%
  set_args(mtry = NULL,
           trees = 500, #Number of trees
           min_n = 2, #minimum number of points in a leaf?
           probability = FALSE,
           importance = 'impurity') %>%
  set_mode('classification')

#recipe
data_rec <- recipe(over25kperc ~ ., data = pitching_clean)

#workflows
data_wf_mtry5 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 5))%>%
  add_recipe(data_rec)
```

```{r}
# fit model
set.seed(123)
data_fit_mtry5 <- fit(data_wf_mtry5, data = pitching_clean)

rf_OOB_output <- function(fit_model, model_label, truth){
  tibble(
    .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), 
    class = truth,
    label = model_label
  
  )
}

#check the output
rf_OOB_output(data_fit_mtry5,data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc))

# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry5,data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc)))

data_rf_OOB_output %>% 
  accuracy(truth = class, estimate = .pred_class) 
```
These outputs are showing the predicted outcomes compared to the actual ones. And an accuracy reading of .85 for the random forest. Since our accuracy is 85% that means we are predicting whether a strike out percentage is above 25% or below 25% with an 85% accuracy. 

```{r}
data_fit_mtry5
rf_OOB_output(data_fit_mtry5, data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc)) %>%
  conf_mat(truth = class, estimate = .pred_class)

sensitivity = 74/(74+34)
specificity = 201/(201+14)
sensitivity
specificity
```
Our sensitivity is .685 and our specificity is .934. Therefore, our model is better at predicting when pitchers are below a 25% strikeout rate than above a 25% strikeout rate - it is better at predicting true negatives than true positives. We think this might be happening because pitchers that have a strikeout rate above 25% are very hard to come by - they are a somewhat elite group, even though the statistic is only slightly above the mlb average. Additionally, our data does not include relief pitchers, who tend to rely more on striking batters out than starting pitchers, who might rely on generating ground balls and soft contact.

```{r}
# evaluate predictor performance based on permutation
model_output <- data_wf_mtry5 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = pitching_clean) %>% 
    extract_fit_engine() 

model_output %>% 
    vip(num_features = 30) + theme_classic()


model_output %>% vip::vi() %>% head()
model_output %>% vip::vi() %>% tail()
```

The most important variables in our model are the average speed,  spin, and break rate of a pitcher's fastball. This makes sense because this is a pitch that all pitchers typically use, and is important for pitchers who try to generate strikeouts by throwing the ball past the batter instead of trying to generate soft contact and ground balls.

## Clustering 


Our goal with clustering is to use fastball pitching data to better explore the pitch and see if there are different types of fastballs. 

Our first step is to create a new dataframe containing only fastball data. Once that is done, we cluster from k = 1 to k = 15 to decide which k value to use.

```{r}
pitching_clean_cluster = pitching_clean %>%
  select(fastball_avg_spin, fastball_avg_speed, fastball_avg_break_x, fastball_avg_break_z)

pitching_cluster <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(pitching_clean_cluster), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, pitching_cluster)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()
```

Using the elbow method, we choose k = 3 as our number of clusters. We choose this because the total within-cluster sum of squares drop off begins to level out after 3.

```{r}
set.seed(123)
kclust_k6 = kmeans(scale(pitching_clean_cluster), centers = 3)
```

```{r}
pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_x, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) +
  geom_point() +
  xlab("Fastball Average Horizontal Break (Inches)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") + 
  theme_classic()
```

We first plot the average spin of the fastball vs the average horizontal break while coloring by cluster. We can see two clear clusters in this plot, pitches with break greater than 0, and pitches with break less than 0. Within each group the spin varies. This shows us that spin does not have a large effect on horizontal break, and that most pitchers' fastball move vertically, with very little pitchers located around 0 horizontal break. This plot motivates a deeper dive into the data as we see that cluster 2 and 3 are on top of eachother. 

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_z, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) +
  geom_point() +
  xlab("Fastball Average Vertical Break (Inches)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") +
  theme_classic()
```

Our next plot shows average spin vs the average vertical break. We see a better grouping of the clusters. With group 3 having a larger horizontal break, group 1 being more in the middle, and group 2 having the lowest vertical break. This is interesting because we see a clear difference between groups 2 and 3 in this plot. Group 3 has more vertical break whereas group 2 has the least vertical break.

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_speed, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) + 
  geom_point() +
  xlab("Fastball Average Speed (MPH)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") +
  theme_classic()

```

Our next graphic plots average spin with speed to see if we can see a clear clustering based on spin and speed. We see a similar result as the previous graph. Group 3 throws the hardest fastballs, followed by group 1, and group 2 being the slowest. We again see another interesting distinction between group 3 and group 2. Group 3 has both a higher average speed and vertical break whereas group 2 has the slowest average fastball and smallest vertical break.

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_x, fastball_avg_break_z, color = as.factor(kclust_k6$cluster))) + 
  geom_point() + 
  xlab("Fastball Average Horizontal Break (Inches)") +
  ylab("Fastball Average Vertical Break (Inches)") +
  labs(color = "Cluster") +
  theme_classic()
```

Our final plot shows a distinct 3 groups. Group 3 is has both a high vertical break and low horizontal break, group 2 has both low vertical and horizontal break, and group 1 has a high horizontal break. 

We can see that there are in fact 3 distinct types of fastball contained in our data. This corresponds with the type of pitches pitchers throw. Group 3 is the traditional 4-seam fastball which is thrown harder with more vertical break. Group 2 is a 2-seam/sinker which is thrown with more horizontal break but less speed. And group 1 is the cutter, which can range in velocities but moves in teh opposite direction of the 4-seam or 2-seam. 
