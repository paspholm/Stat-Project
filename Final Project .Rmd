---
title: "Final Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rvest)
tidymodels_prefer()
library(mosaic)
library(glmnet)
library(probably)
library(vip)
conflicted::conflict_prefer("vi", "vip")

load("pitching_clean.rdata")
```

## LASSO OLS Model 


```{r}
pitching1 = read.csv("stats.csv")


pitching_reg_data <- pitching1 %>%
  select(c(1,2,7,34,35,36,118:138)) %>%
  select(-c(1,2)) %>%
  select(!contains("formatted")) %>%
  drop_na()
```


```{r}
set.seed(123)
pitching_cv_10 <- vfold_cv(pitching_reg_data, v = 10)
```

```{r}
#ols  
lm_spec <- 
    linear_reg() %>% # this is the type of model we are fitting
    set_engine(engine = 'lm') %>% # you'll learn other engines to fit the model
    set_mode('regression') 

#lasso model
lm_lasso_spec <- 
  linear_reg()%>%
  set_args(mixture = 1, penalty = tune())%>%
  set_engine(engine = 'glmnet')%>%
  set_mode('regression')
```

```{r}
# recipes & workflows OLS
pitcher_rec <- recipe(p_k_percent ~ ., data = pitching_reg_data) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())#remove predictors with near zero variablility

pitcher_wf <- workflow() %>%
  add_recipe(pitcher_rec)%>%
  add_model(lm_spec)

#recipe and workflow for LASSO
pitcher_rec_LASSO <-
  recipe(p_k_percent ~ ., data = pitching_reg_data) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())%>%#remove predictors with near zero variablility
  step_normalize(all_numeric_predictors())%>%#normalize all predictors 
  step_corr(all_numeric_predictors())


# Lasso Model Spec with tune
pitcher_wf_LASSO <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 
```


```{r}
# fit & tune models

# Workflow (Recipe + Model)
lasso_wf_tune <- workflow() %>% 
  add_recipe(pitcher_rec_LASSO) %>%
  add_model(pitcher_wf_LASSO) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 3)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning hyperparameters
  lasso_wf_tune, # workflow
  resamples = pitching_cv_10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

autoplot(tune_output)+theme_classic()
```

MAE and RMSE plotted against the amount of regularization, otherwise known as the LASSO penalty term. These plots show that as the penalty term increases, thus decreasing the amount of predictors, the cross validated error in our model will increase. At a certain point, after a penalty of 1, the error flatlines, meaning all predictors have been eliminated at this point. Our penalty of choice is probably around 0.3. We can confirm this by extracting the best penalty(shown in the code below), but also this is logical because going too low on the penalty will create problems with overfitting and going too high will have high training error. 


```{r}
#  calculate/collect CV metrics
pitching_cv_metrics <-fit_resamples(pitcher_wf, 
    resamples = pitching_cv_10, 
    metrics = metric_set(rmse,rsq, mae)
    ) %>%
    collect_metrics(summarize = TRUE) #CV Metrics (averages over the 10 folds)

pitching_cv_metrics

#lasso metrics

best_se_penalty <- select_by_one_std_err(tune_output, metric = 'mae', desc(penalty))

final_wf<- finalize_workflow(lasso_wf_tune, best_se_penalty)

final_fit <- fit(final_wf, data = pitching_reg_data)

tidy(final_fit)
```
The OLS model performs somewhat poorly, explaining only 50% of the variance in the data and having high error metrics. While this is not an unusable model, it would be desirable to have better test metrics. Residual plots could lead us to choosing nonlinear models or perhaps using clustering techniques. 

The LASSO output shows the predictors that are included in the final lasso model. The coefficients listed are the result of the LASSO penalty algorithm, where many coefficients are set to 0. This means that the addition of that predictor in the model did not decrease the error enough to justify the inclusion and corresponing increase in the penalty term. 
```{r}
#evaluating with residuals
pitcher_lasso_output <- final_fit %>%
  predict(new_data = pitching_reg_data)%>%
  bind_cols(pitching_reg_data)%>%
  mutate(resids = p_k_percent - .pred)

pitcher_lasso_output%>%
  rsq(truth = p_k_percent, estimate = .pred)

pitching_cv_mod <- fit(pitcher_wf, data = pitching_reg_data)

pitcher_cv_output <- pitching_cv_mod %>%
  predict(new_data = pitching_reg_data)%>%
  bind_cols(pitching_reg_data)%>%
  mutate(resids = p_k_percent - .pred)


```

```{r}
#residual plots OLS
ggplot(data = pitcher_cv_output, aes(x = p_k_percent, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
  
pitcher_cv_output%>%
  ggplot(aes(x = fastball_avg_spin, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_cv_output%>%
  ggplot(aes(x = pitch_count_fastball, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_cv_output%>%
  ggplot(aes(x = fastball_avg_speed, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
```

Residual plots from the OLS cross validated model. These plots show generally good residual plots, but the fastball count versus residual plot shows some heteroscedasticity. It seems that there is generally more error at lower numbers of fastball thrown per pitcher. 
```{r}
#residual plots lasso

  ggplot(data = pitcher_lasso_output, aes(x = p_k_percent, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
  
pitcher_lasso_output%>%
  ggplot(aes(x = fastball_avg_spin, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_lasso_output%>%
  ggplot(aes(x = pitch_count_fastball, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_lasso_output%>%
  ggplot(aes(x = fastball_avg_speed, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
```
The residual plots for the LASSO regression show a similar trend, with a slightly tighter fit and higher R^2 than the normal ols model. 


From this point, we think that testing nonlinear or classification model to decrease error is the best course of action. While the linear regression models are not useless, the error estimates are simply too high for them to be legitimate predictive models that can be used in the future. 




## GAMS and Splines
In this section, we are fitting a generalized additive model to account for non-linearity in a regression framework for this data.
```{r}
pitching_data <- pitching1 %>%
  select(c(1,2,7,34,35,36,118:138)) %>%
  select(-c(1,2)) %>%
  select(!contains("formatted")) %>%
  drop_na()

gam_spec <-
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression')

fit_gam_model <- gam_spec %>%
  fit(p_k_percent ~ s(pitch_count_offspeed) + s(pitch_count_fastball) + s(pitch_count_breaking) + s(fastball_avg_speed) + s(fastball_avg_spin) + s(fastball_avg_break_x) + s(fastball_avg_break_z) + s(fastball_avg_break) + s(fastball_range_speed) + s(offspeed_avg_speed) + s(offspeed_avg_spin) + s(offspeed_avg_break_x) + s(offspeed_avg_break_z) + s(offspeed_avg_break) + s(offspeed_range_speed) + s(breaking_avg_speed) + s(breaking_avg_spin) + s(breaking_avg_break_x) + s(breaking_avg_break_z) + s(breaking_avg_break) + s(breaking_range_speed), data = pitching_data)
```
When deciding the number of knots, the plots show that the default number of knots is satisfactory for all variables. In the residuals vs. theoretical quantities plot, the values stay close to the line, indicating that there is approximately a normal distribution. There is also no real pattern in the residuals vs. linear predictors, and the histogram looks like a somewhat normal distribution. We also observe a clear positive trend with the response vs. fitted values, so this model somewhat predicts accurately. 
```{r}
# check to see if number of knots is good
par(mfrow=c(2,2))
fit_gam_model %>% pluck('fit') %>% mgcv::gam.check()
# I think this is good based on the in-class activity

# summary
fit_gam_model %>% pluck('fit') %>% summary()
```
However, there are many variables where splines may not apply. Variables such as offspeed_avg_speed and fastball_avg_speed have an edf value of 1.000, indicating linearity. Also other variables such as pitch_count_offspeed have high p-values, indicating that their splines are not satisfactory or significant enough to include in the model - they might have better predictive capabilities as a linear coefficient.

```{r}
fit_gam_model %>% pluck('fit') %>% plot( all.terms = TRUE, pages = 3)
```
These plots are another indication of which variables are better used as splines compared to linear models for the predictor. By showing the shape of the predictor, we can visually show which predictors have more curvature and are better fits for spline models due to their nonlinearity.
```{r}
lm_spec <-
 linear_reg() %>%
 set_engine(engine = 'lm') %>%
 set_mode('regression')

set.seed(34) 
data_cv10 <- pitching_data %>%
 vfold_cv(v = 10)

pitching_rec <- recipe(p_k_percent ~ pitch_count_offspeed+ pitch_count_fastball + pitch_count_breaking + fastball_avg_speed + fastball_avg_spin + fastball_avg_break_x + fastball_avg_break_z + fastball_avg_break + fastball_range_speed + offspeed_avg_speed + offspeed_avg_spin + offspeed_avg_break_x + offspeed_avg_break_z + offspeed_avg_break + offspeed_range_speed + breaking_avg_speed + breaking_avg_spin + breaking_avg_break_x + breaking_avg_break_z + breaking_avg_break + breaking_range_speed, data=pitching_data)

# for all variables with edf over 3 we put them into our model as a spline 
spline_rec <- pitching_rec %>% 
  step_ns(fastball_avg_break_x, deg_free = 5)%>%
  step_ns(fastball_avg_break_z, deg_free=4)%>% 
  step_ns(offspeed_avg_break_x, deg_free=4)%>% 
  step_ns(offspeed_range_speed, deg_free=4)%>% 
  step_ns(breaking_range_speed,deg_free=4)

pitching_wf<- workflow()%>% 
  add_model(lm_spec)%>%
  add_recipe(pitching_rec)

spline_wf<- workflow()%>% 
  add_model(lm_spec)%>%
  add_recipe(spline_rec)

fit_resamples(
  pitching_wf, 
  resamples=data_cv10,
  metrics=metric_set(mae,rmse,rsq)
)%>% collect_metrics() 

fit_resamples(
  spline_wf, 
  resamples=data_cv10,
  metrics=metric_set(mae,rmse,rsq)
)%>% collect_metrics() 
```
Ultimately, by selectively using splines for predictors with an edf of three or greater, our model performs better than an OLS model while using cross-validation to not overfit the model to the data. Comparatively, the GAM model has lower maes and rmses than the OLS model of 2.8 and 3.59 compared to 3.05 and 3.84. It also has a better r-squared of .567 compared to .511, so all of the chosen metrics indicate that accounting for non-linearity is constructive in this context, even though only above 50% of the variation is explained.


## Logistic Regression: Question can we predict strikeout percentage with pitching metrics?  
=======


## Logistic Regression: Question: can we predict strikeout percentage with pitching metrics?  



```{r}
set.seed(69)

pitching_cv11 = vfold_cv(pitching_clean, v = 11)

# model specification
logistic_lasso_spec_tune = logistic_reg() %>%
  set_engine("glmnet") %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_mode('classification')

# recipe
logistic_rec = recipe(over25kperc ~ ., data = pitching_clean[-c(1,2)]) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

# workflow
log_lasso_wf = workflow() %>%
  add_recipe(logistic_rec) %>%
  add_model(logistic_lasso_spec_tune)

# Tune Model
penalty_grid = grid_regular(
  penalty(range = c(-5, -2)), 
  levels = 30
)

tune_output = tune_grid(
  log_lasso_wf,
  resamples = pitching_cv11,
  metrics = metric_set(roc_auc),
  control = control_resamples(save_pred = TRUE, event_level = "second"),
  grid = penalty_grid
)

autoplot(tune_output) + theme_classic()

best_se_penalty = select_by_one_std_err(tune_output, metric = "roc_auc", desc(penalty))

best_se_penalty
```

```{r}
#final fit
final_fit_se = finalize_workflow(log_lasso_wf, best_se_penalty)%>%
  fit(data = pitching_clean)

tidy(final_fit_se)
```

```{r}
#variable importance 

glmnet_output <- final_fit_se %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    this_coeff_path <- bool_predictor_exclude[row,]
    if(mosaic::sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{
    return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)}
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```

```{r}
tune_output%>% 
  collect_metrics()%>%
  filter(penalty == best_se_penalty %>%pull(penalty))

pitching_clean%>%
  count(over25kperc)

logistic_output <-  pitching_clean %>%
  bind_cols(predict(final_fit_se, new_data = pitching_clean, type = 'prob')) 

logistic_output%>%
  ggplot(aes(y = .pred_1, x = over25kperc))+
  geom_boxplot()+theme_classic()

```

```{r}
#hard predicitons
logistic_output %>%
  ggplot(aes(x = over25kperc, y = .pred_0))+
  geom_boxplot()+
  geom_hline(yintercept = 0.69, col = "red")+
  theme_classic()

logistic_output<-logistic_output%>%
  mutate(.pred_class = make_two_class_pred(.pred_0, levels(over25kperc), threshold = .69))
  
head(logistic_output)

logistic_output%>%
  count(over25kperc, .pred_class)

logistic_output %>%
  conf_mat(truth = over25kperc, estimate = .pred_class)
```


##Decision Trees/Random Forests 
### Question: Predicting Strikeout precentage over 25% with random forests and decsions trees. 
=======
```{r}
set.seed(123)

#model specification
rf_spec <- rand_forest()%>%
  set_engine(engine = 'ranger') %>%
  set_args(mtry = NULL,
           trees = 500, #Number of trees
           min_n = 2, #minimum number of points in a leaf?
           probability = FALSE,
           importance = 'impurity') %>%
  set_mode('classification')

#recipe
data_rec <- recipe(over25kperc ~ ., data = pitching_clean)

#workflows
data_wf_mtry5 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 5))%>%
  add_recipe(data_rec)
```

```{r}
# fit model
set.seed(123)
data_fit_mtry5 <- fit(data_wf_mtry5, data = pitching_clean)

rf_OOB_output <- function(fit_model, model_label, truth){
  tibble(
    .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), 
    class = truth,
    label = model_label
  
  )
}

#check the output
rf_OOB_output(data_fit_mtry5,data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc))

# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry5,data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc)))

data_rf_OOB_output %>% 
  accuracy(truth = class, estimate = .pred_class) 
```
These outputs are showing the predicted outcomes compared to the actual ones. And an accuracy reading of .85 for the random forest. Since our accuracy is 85% that means we are predicting whether a strike out percentage is above 25% or below 25% with an 85% accuracy. 

```{r}
data_fit_mtry5
rf_OOB_output(data_fit_mtry5, data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc)) %>%
  conf_mat(truth = class, estimate = .pred_class)

sensitivity = 74/(74+34)
specificity = 201/(201+14)
sensitivity
specificity
```
Our sensitivity is .685 and our specificity is .934. Therefore, our model is better at predicting when pitchers are below a 25% strikeout rate than above a 25% strikeout rate - it is better at predicting true negatives than true positives. We think this might be happening because pitchers that have a strikeout rate above 25% are very hard to come by - they are a somewhat elite group, even though the statistic is only slightly above the mlb average. Additionally, our data does not include relief pitchers, who tend to rely more on striking batters out than starting pitchers, who might rely on generating ground balls and soft contact.

```{r}
# evaluate predictor performance based on permutation
model_output <- data_wf_mtry5 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = pitching_clean) %>% 
    extract_fit_engine() 

model_output %>% 
    vip(num_features = 30) + theme_classic()


model_output %>% vip::vi() %>% head()
model_output %>% vip::vi() %>% tail()
```

The most important variables in our model are the average speed,  spin, and break rate of a pitcher's fastball. This makes sense because this is a pitch that all pitchers typically use, and is important for pitchers who try to generate strikeouts by throwing the ball past the batter instead of trying to generate soft contact and ground balls.

## Clustering 


Our goal with clustering is to use fastball pitching data to better explore the pitch and see if there are different types of fastballs. 

Our first step is to create a new dataframe containing only fastball data. Once that is done, we cluster from k = 1 to k = 15 to decide which k value to use.

```{r}
pitching_clean_cluster = pitching_clean %>%
  select(fastball_avg_spin, fastball_avg_speed, fastball_avg_break_x, fastball_avg_break_z)

pitching_cluster <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(pitching_clean_cluster), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, pitching_cluster)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()
```

Using the elbow method, we choose k = 3 as our number of clusters. We choose this because the total within-cluster sum of squares drop off begins to level out after 3.

```{r}
set.seed(123)
kclust_k6 = kmeans(scale(pitching_clean_cluster), centers = 3)
```

```{r}
pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_x, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) +
  geom_point() +
  xlab("Fastball Average Horizontal Break (Inches)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") + 
  theme_classic()
```

We first plot the average spin of the fastball vs the average horizontal break while coloring by cluster. We can see two clear clusters in this plot, pitches with break greater than 0, and pitches with break less than 0. Within each group the spin varies. This shows us that spin does not have a large effect on horizontal break, and that most pitchers' fastball move vertically, with very little pitchers located around 0 horizontal break. This plot motivates a deeper dive into the data as we see that cluster 2 and 3 are on top of eachother. 

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_z, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) +
  geom_point() +
  xlab("Fastball Average Vertical Break (Inches)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") +
  theme_classic()
```

Our next plot shows average spin vs the average vertical break. We see a better grouping of the clusters. With group 3 having a larger horizontal break, group 1 being more in the middle, and group 2 having the lowest vertical break. This is interesting because we see a clear difference between groups 2 and 3 in this plot. Group 3 has more vertical break whereas group 2 has the least vertical break.

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_speed, y = fastball_avg_spin, color = as.factor(kclust_k6$cluster))) + 
  geom_point() +
  xlab("Fastball Average Speed (MPH)") +
  ylab("Fastball Average Spin (RPM)") +
  labs(color = "Cluster") +
  theme_classic()

```

Our next graphic plots average spin with speed to see if we can see a clear clustering based on spin and speed. We see a similar result as the previous graph. Group 3 throws the hardest fastballs, followed by group 1, and group 2 being the slowest. We again see another interesting distinction between group 3 and group 2. Group 3 has both a higher average speed and vertical break whereas group 2 has the slowest average fastball and smallest vertical break.

```{r}

pitching_clean_cluster %>%
  ggplot(aes(x = fastball_avg_break_x, fastball_avg_break_z, color = as.factor(kclust_k6$cluster))) + 
  geom_point() + 
  xlab("Fastball Average Horizontal Break (Inches)") +
  ylab("Fastball Average Vertical Break (Inches)") +
  labs(color = "Cluster") +
  theme_classic()
```

Our final plot shows a distinct 3 groups. Group 3 is has both a high vertical break and low horizontal break, group 2 has both low vertical and horizontal break, and group 1 has a high horizontal break. 

We can see that there are in fact 3 distinct types of fastball contained in our data. This corresponds with the type of pitches pitchers throw. Group 3 is the traditional 4-seam fastball which is thrown harder with more vertical break. Group 2 is a 2-seam/sinker which is thrown with more horizontal break but less speed. And group 1 is the cutter, which can range in velocities but moves in teh opposite direction of the 4-seam or 2-seam. 
