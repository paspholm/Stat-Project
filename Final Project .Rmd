---
title: "Final Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rvest)
tidymodels_prefer()
library(mosaic)
library(glmnet)
library(probably)
library(vip)
conflicted::conflict_prefer("vi", "vip")

load("pitching_clean.rdata")
```

## LASSO OLS Model 


```{r}
pitching1 = read.csv("stats.csv")


pitching_reg_data <- pitching1 %>%
  select(c(1,2,7,34,35,36,118:138)) %>%
  select(-c(1,2)) %>%
  select(!contains("formatted")) %>%
  drop_na()
```


```{r}
set.seed(123)
pitching_cv_10 <- vfold_cv(pitching_reg_data, v = 10)
```

## Research question for OLS and LASSO linear regression: we are trying to predict strikeout percentage for a pitcher based on only pitch movement, spin and velocity. 

```{r}
#ols  
lm_spec <- 
    linear_reg() %>% # this is the type of model we are fitting
    set_engine(engine = 'lm') %>% # you'll learn other engines to fit the model
    set_mode('regression') 

#lasso model
lm_lasso_spec <- 
  linear_reg()%>%
  set_args(mixture = 1, penalty = tune())%>%
  set_engine(engine = 'glmnet')%>%
  set_mode('regression')
```

```{r}
# recipes & workflows OLS
pitcher_rec <- recipe(p_k_percent ~ ., data = pitching_reg_data) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())#remove predictors with near zero variablility

pitcher_wf <- workflow() %>%
  add_recipe(pitcher_rec)%>%
  add_model(lm_spec)

#recipe and workflow for LASSO
pitcher_rec_LASSO <-
  recipe(p_k_percent ~ ., data = pitching_reg_data) %>%
  step_lincomb(all_numeric_predictors())%>% #remove predictors that are combinations of each other
  step_nzv(all_numeric_predictors())%>%#remove predictors with near zero variablility
  step_normalize(all_numeric_predictors())%>%#normalize all predictors 
  step_corr(all_numeric_predictors())


# Lasso Model Spec with tune
pitcher_wf_LASSO <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 
```


```{r}
# fit & tune models

# Workflow (Recipe + Model)
lasso_wf_tune <- workflow() %>% 
  add_recipe(pitcher_rec_LASSO) %>%
  add_model(pitcher_wf_LASSO) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 3)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning hyperparameters
  lasso_wf_tune, # workflow
  resamples = pitching_cv_10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

autoplot(tune_output)+theme_classic()
```

MAE and RMSE plotted against the amount of regularization, otherwise known as the LASSO penalty term. These plots show that as the penalty term increases, thus decreasing the amount of predictors, the cross validated error in our model will increase. At a certain point, after a penalty of 1, the error flatlines, meaning all predictors have been eliminated at this point. Our penalty of choice is probably around 0.3. We can confirm this by extracting the best penalty(shown in the code below), but also this is logical because going too low on the penalty will create problems with overfitting and going too high will have high training error. 


```{r}
#  calculate/collect CV metrics
pitching_cv_metrics <-fit_resamples(pitcher_wf, 
    resamples = pitching_cv_10, 
    metrics = metric_set(rmse,rsq, mae)
    ) %>%
    collect_metrics(summarize = TRUE) #CV Metrics (averages over the 10 folds)

pitching_cv_metrics

#lasso metrics

best_se_penalty <- select_by_one_std_err(tune_output, metric = 'mae', desc(penalty))

final_wf<- finalize_workflow(lasso_wf_tune, best_se_penalty)

final_fit <- fit(final_wf, data = pitching_reg_data)

tidy(final_fit)
```
The OLS model performs somewhat poorly, explaining only 50% of the variance in the data and having high error metrics. While this is not an unusable model, it would be desirable to have better test metrics. Residual plots could lead us to choosing nonlinear models or perhaps using clustering techniques. 

The LASSO output shows the predictors that are included in the final lasso model. The coefficients listed are the result of the LASSO penalty algorithm, where many coefficients are set to 0. This means that the addition of that predictor in the model did not decrease the error enough to justify the inclusion and corresponing increase in the penalty term. 
```{r}
#evaluating with residuals
pitcher_lasso_output <- final_fit %>%
  predict(new_data = pitching_reg_data)%>%
  bind_cols(pitching_reg_data)%>%
  mutate(resids = p_k_percent - .pred)

pitcher_lasso_output%>%
  rsq(truth = p_k_percent, estimate = .pred)

pitching_cv_mod <- fit(pitcher_wf, data = pitching_reg_data)

pitcher_cv_output <- pitching_cv_mod %>%
  predict(new_data = pitching_reg_data)%>%
  bind_cols(pitching_reg_data)%>%
  mutate(resids = p_k_percent - .pred)


```

```{r}
#residual plots OLS
ggplot(data = pitcher_cv_output, aes(x = p_k_percent, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
  
pitcher_cv_output%>%
  ggplot(aes(x = fastball_avg_spin, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_cv_output%>%
  ggplot(aes(x = pitch_count_fastball, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_cv_output%>%
  ggplot(aes(x = fastball_avg_speed, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
```

Residual plots from the OLS cross validated model. These plots show generally good residual plots, but the fastball count versus residual plot shows some heteroscedasticity. It seems that there is generally more error at lower numbers of fastball thrown per pitcher. 

```{r}
#residual plots lasso

  ggplot(data = pitcher_lasso_output, aes(x = p_k_percent, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
  
pitcher_lasso_output%>%
  ggplot(aes(x = fastball_avg_spin, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_lasso_output%>%
  ggplot(aes(x = pitch_count_fastball, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()

pitcher_lasso_output%>%
  ggplot(aes(x = fastball_avg_speed, y = resids))+
  geom_point()+
  geom_smooth(method = 'lm')+ 
  geom_hline(yintercept = 0, color = "red")+
  theme_classic()
```
The residual plots for the LASSO regression show a similar trend, with a slightly tighter fit and higher R^2 than the normal ols model. 


From this point, we think that testing nonlinear or classification model to decrease error is the best course of action. While the linear regressino models are not useless, the error estimates are simply too high for them to be legitimate predictive models that can be used in the future. 



## GAMS and Splines NEED TO DO 
## Research question for Logistic Regression: can we predict whether a pitcher has over or under a certain treshold (25%) strikeout percentage with velocity, movement and spin data? 

```{r}
set.seed(69)

pitching_cv11 = vfold_cv(pitching_clean, v = 11)

# model specification
logistic_lasso_spec_tune = logistic_reg() %>%
  set_engine("glmnet") %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_mode('classification')

# recipe
logistic_rec = recipe(over25kperc ~ ., data = pitching_clean[-c(1,2)]) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

# workflow
log_lasso_wf = workflow() %>%
  add_recipe(logistic_rec) %>%
  add_model(logistic_lasso_spec_tune)

# Tune Model
penalty_grid = grid_regular(
  penalty(range = c(-5, 3)), 
  levels = 30
)

tune_output = tune_grid(
  log_lasso_wf,
  resamples = pitching_cv11,
  metrics = metric_set(roc_auc),
  control = control_resamples(save_pred = TRUE, event_level = "second"),
  grid = penalty_grid
)

autoplot(tune_output) + theme_classic()

best_se_penalty = select_by_one_std_err(tune_output, metric = "roc_auc", desc(penalty))

best_se_penalty
```

For the LASSO in logistic regression, we used the ROC AUC metric for variable selection. This shows that as the lasso penalty increases, the AUC decreases. This means that the inclusion of more predictors is good for the model. 

With the LASSO penalty being used, the mean ROC AUC for our models was 0.87. This is metric is very high and indicates that our model is very accurate.


```{r}
#final fit
final_fit_se = finalize_workflow(log_lasso_wf, best_se_penalty)%>%
  fit(data = pitching_clean)

tidy(final_fit_se)
```

The output for our final model with all coefficients and predictors listed with the corresponding cross validated penalty. This shows us what predictors are included in our model. It seems that generally, the predictors are similar to those in the LASSO regression model. However, the performance of logistic regression is much better than our linear regression model. This indicates that our data may perform better under a classification framework rather than regression. 

```{r}
#variable importance 

glmnet_output <- final_fit_se %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    this_coeff_path <- bool_predictor_exclude[row,]
    if(mosaic::sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{
    return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)}
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```
Our variable importance table. This lists, in order, the variables that are most to least influential on our final model. Fastball avg speed and spin are equally the most important variables. This indicates that fastballs that are thrown hard with high spin rates are the biggest predictor of if someone has over 25% strikeout rate. 

```{r}
tune_output%>% 
  collect_metrics()%>%
  filter(penalty == best_se_penalty %>%pull(penalty))

pitching_clean%>%
  count(over25kperc)

logistic_output <-  pitching_clean %>%
  bind_cols(predict(final_fit_se, new_data = pitching_clean, type = 'prob')) 

logistic_output%>%
  ggplot(aes(y = .pred_0, x = over25kperc))+
  geom_boxplot()+theme_classic()

```
We can see the boxplots of our predictions for pitchers over and under 25% strikeout rate. There are a few outliers in the no column, meaning that there is a small group that does not strikeout many batters despite having metrics on their pitches that indicate a high strikeout rate. This could be due many factors, such as poor location, playing in a hitter friendly environment or pitch selection and game calling. 


```{r}
#hard predicitons
logistic_output %>%
  ggplot(aes(x = over25kperc, y = .pred_0))+
  geom_boxplot()+
  geom_hline(yintercept = 0.69, col = "red")+
  theme_classic()

logistic_output<-logistic_output%>%
  mutate(.pred_class = make_two_class_pred(.pred_0, levels(over25kperc), threshold = .69))
  
head(logistic_output)

logistic_output%>%
  count(over25kperc, .pred_class)

logistic_output %>%
  conf_mat(truth = over25kperc, estimate = .pred_class)
```


```{r}
#hard predicitons
logistic_output %>%
  ggplot(aes(x = over25kperc, y = .pred_0))+
  geom_boxplot()+
  geom_hline(yintercept = 0.69, col = "red")+
  theme_classic()


log_metrics <- metric_set(accuracy, sens, yardstick::spec)

logistic_output %>%
  mutate(.pred_class = make_two_class_pred(.pred_0, levels(over25kperc), threshold= 0.65))%>%
  log_metrics(truth = over25kperc, estimate = .pred_class, event_level = 'second')

logistic_output %>%
  roc_curve(over25kperc, .pred_0, event_level = 'second')%>%
  autoplot()

threshold_output <- logistic_output %>%
  threshold_perf(truth = over25kperc, estimate = .pred_0, threshold = seq(0,1, by=0.01))

threshold_output %>%
  filter(.metric == "j_index")%>%
  ggplot(aes(x = .threshold, y = .estimate))+
  geom_line()+
  theme_classic()

threshold_output%>%
  filter(.metric == 'j_index')%>%
  arrange(desc(.estimate))
```
For our final logistic output model making a hard prediction, we use the j_index metric to choose the best threshold level in order to optimize accuracy, specificity and sensitivity. These metrics are output here along with the ROC_AUC graph and the boxplot with our hard prediction line superimposed. Overall, this model performs fairly well but could still be better. To some degree, we believe we are lacking data in certain areas that explain variance in strikout rate. Release height and angle could be important and deception is nearly impossible to account for quantitatively, but is generally accepted as an attribute that certain pitcher have, increasing their ability to strike hitters out. 


##Descision Trees/Random Forests 

```{r}
set.seed(123)

#model specification
rf_spec <- rand_forest()%>%
  set_engine(engine = 'ranger') %>%
  set_args(mtry = NULL,
           trees = 500, #Number of trees
           min_n = 2, #minimum number of points in a leaf?
           probability = FALSE,
           importance = 'impurity') %>%
  set_mode('classification')

#recipe
data_rec <- recipe(over25kperc ~ ., data = pitching_clean)

#workflows
data_wf_mtry5 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 5))%>%
  add_recipe(data_rec)
```

```{r}
# fit model
set.seed(123)
data_fit_mtry5 <- fit(data_wf_mtry5, data = pitching_clean)

rf_OOB_output <- function(fit_model, model_label, truth){
  tibble(
    .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), 
    class = truth,
    label = model_label
  
  )
}

#check the output
rf_OOB_output(data_fit_mtry5,data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc))

# Evaluate OOB Metrics
data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry5,data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc)))

data_rf_OOB_output %>% 
  accuracy(truth = class, estimate = .pred_class) 
```

```{r}
data_fit_mtry5
rf_OOB_output(data_fit_mtry5, data_fit_mtry5$over25kperc, pitching_clean %>% pull(over25kperc)) %>%
  conf_mat(truth = class, estimate = .pred_class)

sensitivity = 74/(74+34)
specificity = 201/(201+14)
sensitivity
specificity
```
Our sensitivity is .685 and our specificity is .934. Therefore, our model is better at predicting when pitchers are below a 25% strikeout rate than above a 25% strikeout rate - it is better at predicting true negatives than true positives. We think this might be happening because pitchers that have a strikeout rate above 25% are very hard to come by - they are a somewhat elite group, even though the statistic is only slightly above the mlb average. Additionally, our data does not include relief pitchers, who tend to rely more on striking batters out than starting pitchers, who might rely on generating ground balls and soft contact.

```{r}
# evaluate predictor performance based on permutation
model_output <- data_wf_mtry5 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = pitching_clean) %>% 
    extract_fit_engine() 

model_output %>% 
    vip(num_features = 30) + theme_classic()


model_output %>% vip::vi() %>% head()
model_output %>% vip::vi() %>% tail()
```

The most important variables in our model are the average speed,  spin, and break rate of a pitcher's fastball. This makes sense because this is a pitch that all pitchers typically use, and is important for pitchers who try to generate strikeouts by throwing the ball past the batter instead of trying to generate soft contact and ground balls.
## Clustering 
